{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dafa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# XGBoost Modeling\n",
    "# Outputs (under ./models & ./results & ./submissions)\n",
    "# ===============================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = Path(\"./data\")\n",
    "EMBEDDINGS_DIR = Path(\"./embeddings\")\n",
    "MODELS_DIR = Path(\"./models\")\n",
    "SUBMISSIONS_DIR = Path(\"./submissions\")\n",
    "RESULTS_DIR = Path(\"./results\")\n",
    "\n",
    "TRAIN_PATH = DATA_DIR / \"train_preprocessed.csv\"\n",
    "TEST_PATH  = DATA_DIR / \"test_preprocessed.csv\"\n",
    "\n",
    "# Text artifacts\n",
    "EMBEDDINGS_MODEL = \"all-mpnet-base-v2\"\n",
    "TEXT_PCA_TRAIN = EMBEDDINGS_DIR / EMBEDDINGS_MODEL / \"pca\" / f\"text_pca_train.parquet\"\n",
    "TEXT_PCA_TEST  = EMBEDDINGS_DIR / EMBEDDINGS_MODEL / \"pca\" / f\"text_pca_test.parquet\"\n",
    "TEXT_SEV_OOF   = EMBEDDINGS_DIR / EMBEDDINGS_MODEL / \"severity\" / f\"text_severity_oof.parquet\"\n",
    "TEXT_SEV_TEST  = EMBEDDINGS_DIR / EMBEDDINGS_MODEL / \"severity\" / f\"text_severity_test.parquet\"\n",
    "\n",
    "# Feature flags\n",
    "INCLUDE_IICC = True\n",
    "INCLUDE_TEXT = True\n",
    "\n",
    "# Columns\n",
    "TARGET_COL = \"UltimateIncurredClaimCost\"\n",
    "ID_COL     = \"ClaimNumber\"\n",
    "STRAT_COL  = \"accident_year\"\n",
    "\n",
    "# Optimization / stacking\n",
    "N_INITIAL_POINTS = 10\n",
    "N_CALLS          = 30\n",
    "N_BEST_MODELS    = 4\n",
    "\n",
    "# Split\n",
    "VALID_SIZE = 0.20\n",
    "\n",
    "# Suffixes\n",
    "TEST_SUFFIX = f\"{'IICC' if INCLUDE_IICC else 'noIICC'}_{'TEXT' if INCLUDE_TEXT else 'noTEXT'}_{EMBEDDINGS_MODEL}\"\n",
    "RUN_SUFFIX  = f\"xgb_gp_stack_{TEST_SUFFIX}\"\n",
    "\n",
    "DIR_MODEL = MODELS_DIR / RUN_SUFFIX\n",
    "\n",
    "for p in [DIR_MODEL, SUBMISSIONS_DIR, RESULTS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def rmse_orig(y_true, y_pred):\n",
    "    return root_mean_squared_error(y_true, y_pred)\n",
    "\n",
    "def to_log(y):\n",
    "    return np.log1p(np.maximum(y, 0.0))\n",
    "\n",
    "def from_log(p):\n",
    "    return np.maximum(np.expm1(p), 0.0)\n",
    "\n",
    "def safe_replace_inf(df_like):\n",
    "    df_like.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    return df_like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b615cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 01) Data Loading & Text Merge\n",
    "# ===============================================================\n",
    "def load_data(train_path: Path, test_path: Path):\n",
    "    df = pd.read_csv(train_path)\n",
    "    dft = pd.read_csv(test_path)\n",
    "    return safe_replace_inf(df), safe_replace_inf(dft)\n",
    "\n",
    "def maybe_merge_text(df: pd.DataFrame, dft: pd.DataFrame, include_text: bool):\n",
    "    if not include_text:\n",
    "        return df, dft\n",
    "\n",
    "    pca_tr = pd.read_parquet(TEXT_PCA_TRAIN)\n",
    "    pca_te = pd.read_parquet(TEXT_PCA_TEST)\n",
    "    sev_tr = pd.read_parquet(TEXT_SEV_OOF).rename(columns={\"text_sev_oof\": \"text_sev\"})\n",
    "    sev_te = pd.read_parquet(TEXT_SEV_TEST).rename(columns={\"text_sev_pred\": \"text_sev\"})\n",
    "\n",
    "    df_merged  = df.merge(pca_tr, on=ID_COL, how=\"left\").merge(sev_tr, on=ID_COL, how=\"left\")\n",
    "    dft_merged = dft.merge(pca_te, on=ID_COL, how=\"left\").merge(sev_te, on=ID_COL, how=\"left\")\n",
    "    return df_merged, dft_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5321bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 02) Feature Selection\n",
    "# ===============================================================\n",
    "def select_features(df: pd.DataFrame, include_iicc: bool, include_text: bool):\n",
    "    base_feats = [\n",
    "        \"WeeklyWages\",\"hourly_wage\",\"HoursWorkedPerWeek\",\"DaysWorkedPerWeek\",\n",
    "        \"Age\",\"DependentChildren\",\"DependentsOther\",\n",
    "        \"accident_year\",\"accident_month\",\"accident_dow\",\"accident_hour\",\n",
    "        \"report_delay_days\",\"is_weekend\",\n",
    "    ]\n",
    "    cat_feats = [c for c in [\"Gender\",\"MaritalStatus\",\"PartTimeFullTime\"] if c in df.columns]\n",
    "    flag_feats = [c for c in [\"iicc_is_one_flag\",\"iicc_small_flag\",\"inconsistent_wages_flag\",\"invalid_exposure_flag\"] if c in df.columns]\n",
    "\n",
    "    iicc_feats = []\n",
    "    if include_iicc:\n",
    "        for c in [\"InitialIncurredClaimsCost\",\"logIICC\"]:\n",
    "            if c in df.columns:\n",
    "                iicc_feats.append(c)\n",
    "\n",
    "    text_feats = []\n",
    "    if include_text:\n",
    "        text_feats += [c for c in df.columns if c.startswith(\"text_pca_\")]\n",
    "        for c in [\"desc_char_len\",\"desc_token_len\",\"text_sev\"]:\n",
    "            if c in df.columns:\n",
    "                text_feats.append(c)\n",
    "\n",
    "    feat_cols = base_feats + flag_feats + iicc_feats + text_feats + cat_feats\n",
    "    feat_cols = [c for c in feat_cols if c in df.columns]\n",
    "\n",
    "    numeric_cols = [c for c in feat_cols if c not in cat_feats]\n",
    "    categorical_cols = [c for c in cat_feats if c in df.columns]\n",
    "    return feat_cols, numeric_cols, categorical_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "337ad5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 03) Split & Preprocessor\n",
    "# ===============================================================\n",
    "\n",
    "def split_data(df: pd.DataFrame, strat_col: str, valid_size: float, seed: int):\n",
    "    assert strat_col in df.columns, f\"Missing strat column {strat_col}\"\n",
    "    train_idx, valid_idx = train_test_split(\n",
    "        np.arange(len(df)),\n",
    "        test_size=valid_size,\n",
    "        random_state=seed,\n",
    "        stratify=df[strat_col].astype(int)\n",
    "    )\n",
    "    train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "    valid_df = df.iloc[valid_idx].reset_index(drop=True)\n",
    "    return train_df, valid_df\n",
    "\n",
    "def build_preprocessor(categorical_cols, numeric_cols):\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_cols),\n",
    "            (\"num\", \"passthrough\", numeric_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.0\n",
    "    )\n",
    "\n",
    "def build_design_matrices(preprocessor, feat_cols, train_df, valid_df, test_df):\n",
    "    X_train = preprocessor.fit_transform(train_df[feat_cols])\n",
    "    X_valid = preprocessor.transform(valid_df[feat_cols])\n",
    "    X_test  = preprocessor.transform(test_df[feat_cols])\n",
    "    return X_train, X_valid, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1911470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 04) Bayesian Optimization on Validation\n",
    "# ===============================================================\n",
    "def bayes_opt_xgb(\n",
    "    X_train, y_train_log, X_valid, y_valid, n_calls: int, n_init: int, seed: int\n",
    "):\n",
    "    # Search space\n",
    "    space  = [\n",
    "        Integer(3, 12,     name=\"max_depth\"),\n",
    "        Real(0.01, 0.3,    name=\"learning_rate\"),\n",
    "        Integer(400, 3000, name=\"n_estimators\"),\n",
    "        Real(0.5, 1.0,     name=\"subsample\"),\n",
    "        Real(0.5, 1.0,     name=\"colsample_bytree\"),\n",
    "        Real(1e-8, 50.0,   name=\"min_child_weight\"),\n",
    "        Real(0.0, 10.0,    name=\"reg_alpha\"),\n",
    "        Real(0.0, 20.0,    name=\"reg_lambda\"),\n",
    "        Real(0.0, 10.0,    name=\"gamma\"),\n",
    "    ]\n",
    "\n",
    "    trial_params = []\n",
    "    trial_valid_preds = []\n",
    "    trial_scores = []\n",
    "    y_valid_log = to_log(y_valid)\n",
    "\n",
    "    @use_named_args(space)\n",
    "    def objective(**params):\n",
    "        model = xgb.XGBRegressor(\n",
    "            objective=\"reg:squarederror\",\n",
    "            eval_metric=\"rmse\",\n",
    "            random_state=seed,\n",
    "            n_jobs=os.cpu_count(),\n",
    "            tree_method=\"hist\",\n",
    "            **params\n",
    "        )\n",
    "        model.fit(X_train, y_train_log, eval_set=[(X_valid, y_valid_log)], verbose=False)\n",
    "        yv_pred = from_log(model.predict(X_valid))\n",
    "        score = rmse_orig(y_valid, yv_pred)\n",
    "\n",
    "        trial_params.append(params)\n",
    "        trial_valid_preds.append(yv_pred)\n",
    "        trial_scores.append(score)\n",
    "        return score\n",
    "\n",
    "    res = gp_minimize(\n",
    "        func=objective,\n",
    "        dimensions=space,\n",
    "        n_calls=n_calls,\n",
    "        n_initial_points=n_init,\n",
    "        random_state=seed,\n",
    "        acq_func=\"EI\",\n",
    "        noise=\"gaussian\"\n",
    "    )\n",
    "    return res, trial_params, trial_valid_preds, trial_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8aacee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 05) Stacker fit (RidgeCV) on Validation set\n",
    "# ===============================================================\n",
    "def fit_stacker_ridgecv(P_valid, y_valid):\n",
    "    ridge = RidgeCV(alphas=np.logspace(-3, 3, 13), cv=5, scoring=\"neg_mean_squared_error\")\n",
    "    ridge.fit(P_valid, y_valid)\n",
    "    valid_blend = ridge.predict(P_valid)\n",
    "    return ridge, valid_blend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d78e7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 06) Full-data refit of top models + test prediction\n",
    "# ===============================================================\n",
    "def refit_and_predict_full(\n",
    "    df_full, dft, feat_cols, cat_feats, params_list, seed\n",
    "):\n",
    "    # Full-data preprocessor\n",
    "    preprocessor_full = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), [c for c in cat_feats if c in df_full.columns]),\n",
    "            (\"num\", \"passthrough\", [c for c in feat_cols if c not in cat_feats]),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.0\n",
    "    )\n",
    "    X_full  = preprocessor_full.fit_transform(df_full[feat_cols])\n",
    "    X_testF = preprocessor_full.transform(dft[feat_cols])\n",
    "\n",
    "    y_full   = df_full[TARGET_COL].values\n",
    "    y_full_l = to_log(y_full)\n",
    "\n",
    "    # Fit each top model\n",
    "    test_pred_matrix = []\n",
    "    for params in params_list:\n",
    "        model = xgb.XGBRegressor(\n",
    "            objective=\"reg:squarederror\",\n",
    "            eval_metric=\"rmse\",\n",
    "            random_state=seed,\n",
    "            n_jobs=os.cpu_count(),\n",
    "            tree_method=\"hist\",\n",
    "            **params\n",
    "        )\n",
    "        model.fit(X_full, y_full_l, verbose=False)\n",
    "        yp = from_log(model.predict(X_testF))\n",
    "        test_pred_matrix.append(yp)\n",
    "\n",
    "    P_test = np.column_stack(test_pred_matrix).astype(np.float64)\n",
    "\n",
    "    # Return everything needed to save for inference\n",
    "    bundle = {\n",
    "        \"preprocessor_full\": preprocessor_full,\n",
    "        \"feat_cols\": feat_cols,\n",
    "        \"cat_feats\": cat_feats,\n",
    "        \"top_params\": params_list,\n",
    "        \"seed\": seed,\n",
    "    }\n",
    "    return P_test, bundle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9db291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 153\n",
      "Sample: ['WeeklyWages', 'hourly_wage', 'HoursWorkedPerWeek', 'DaysWorkedPerWeek', 'Age', 'DependentChildren', 'DependentsOther', 'accident_year', 'accident_month', 'accident_dow', 'accident_hour', 'report_delay_days', 'is_weekend', 'iicc_is_one_flag', 'iicc_small_flag'] ...\n",
      "Train/Valid sizes: 43200 10800\n",
      "X shapes: (43200, 158) (10800, 158) (36000, 158)\n",
      "Best validation RMSE: 22732.767361550657\n",
      "Best params: [np.int64(4), 0.1529638068018135, np.int64(503), 0.8436401141630159, 0.7629446361427012, 1e-08, 2.5725735617941172, 10.727608169367786, 0.0]\n",
      "Top trials (score):\n",
      "1: 22732.7674 | params: {'max_depth': np.int64(4), 'learning_rate': 0.1529638068018135, 'n_estimators': np.int64(503), 'subsample': 0.8436401141630159, 'colsample_bytree': 0.7629446361427012, 'min_child_weight': 1e-08, 'reg_alpha': 2.5725735617941172, 'reg_lambda': 10.727608169367786, 'gamma': 0.0}\n",
      "2: 22740.2462 | params: {'max_depth': np.int64(3), 'learning_rate': 0.07695920943042321, 'n_estimators': np.int64(1027), 'subsample': 0.8416317594127292, 'colsample_bytree': 0.8049983288913105, 'min_child_weight': 41.65974558847628, 'reg_alpha': 1.733646535077721, 'reg_lambda': 7.8212121514648185, 'gamma': 1.8223608778806235}\n",
      "3: 22748.0985 | params: {'max_depth': np.int64(4), 'learning_rate': 0.08755938580721488, 'n_estimators': np.int64(400), 'subsample': 1.0, 'colsample_bytree': 0.9900791504632144, 'min_child_weight': 50.0, 'reg_alpha': 1.9249721772514996, 'reg_lambda': 1.4242725498636442, 'gamma': 0.0}\n",
      "4: 22767.7120 | params: {'max_depth': np.int64(3), 'learning_rate': 0.16218465147493288, 'n_estimators': np.int64(1440), 'subsample': 0.5233328316068078, 'colsample_bytree': 0.9868777594207296, 'min_child_weight': 11.638567029187502, 'reg_alpha': 0.9060643453282081, 'reg_lambda': 12.367720186661748, 'gamma': 3.8246199126716283}\n",
      "Ridge weights: [0.33730115 0.34523278 0.14051235 0.40468665] | intercept: 849.0712505033971\n",
      "Stacked VALID RMSE: 22326.431599347048\n",
      "Saved full training predictions: results\\train_preds_xgb_gp_stack_IICC_TEXT_all-mpnet-base-v2.csv\n",
      "Saved submission: submissions\\submission_xgb_gp_stack_IICC_TEXT_all-mpnet-base-v2.csv\n",
      "Saved model: models\\xgb_gp_stack_IICC_TEXT_all-mpnet-base-v2\\model.pkl\n",
      "Saved config: models\\xgb_gp_stack_IICC_TEXT_all-mpnet-base-v2\\config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# 07) Orchestration\n",
    "# ===============================================================\n",
    "\n",
    "# 1) Load & merge text\n",
    "df, dft = load_data(TRAIN_PATH, TEST_PATH)\n",
    "df, dft = maybe_merge_text(df, dft, INCLUDE_TEXT)\n",
    "\n",
    "# 2) Select features\n",
    "feat_cols, numeric_cols, categorical_cols = select_features(df, INCLUDE_IICC, INCLUDE_TEXT)\n",
    "\n",
    "print(\"Total features:\", len(feat_cols))\n",
    "print(\"Sample:\", feat_cols[:15], \"...\")\n",
    "\n",
    "# 3) Split\n",
    "train_df, valid_df = split_data(df, STRAT_COL, VALID_SIZE, SEED)\n",
    "y_train = train_df[TARGET_COL].values\n",
    "y_valid = valid_df[TARGET_COL].values\n",
    "\n",
    "# 4) Preprocessor & matrices\n",
    "pre = build_preprocessor(categorical_cols, numeric_cols)\n",
    "X_train, X_valid, X_test = build_design_matrices(pre, feat_cols, train_df, valid_df, dft)\n",
    "y_train_log = to_log(y_train)\n",
    "y_valid_log = to_log(y_valid)\n",
    "\n",
    "print(\"Train/Valid sizes:\", len(train_df), len(valid_df))\n",
    "print(\"X shapes:\", X_train.shape, X_valid.shape, X_test.shape)\n",
    "\n",
    "# 5) gp.minimize on VALID\n",
    "res, trial_params, trial_valid_preds, trial_scores = bayes_opt_xgb(\n",
    "    X_train, y_train_log, X_valid, y_valid, n_calls=N_CALLS, n_init=N_INITIAL_POINTS, seed=SEED\n",
    ")\n",
    "print(\"Best validation RMSE:\", res.fun)\n",
    "print(\"Best params:\", res.x)\n",
    "\n",
    "# 6) Select best K trials and stack\n",
    "order = np.argsort(np.array(trial_scores))\n",
    "top_k = order[:N_BEST_MODELS]\n",
    "print(\"Top trials (score):\")\n",
    "for i, idx in enumerate(top_k, 1):\n",
    "    print(f\"{i}: {trial_scores[idx]:.4f} | params: {trial_params[idx]}\")\n",
    "\n",
    "P_valid = np.column_stack([trial_valid_preds[idx] for idx in top_k]).astype(np.float64)\n",
    "ridge, valid_blend = fit_stacker_ridgecv(P_valid, y_valid)\n",
    "print(\"Ridge weights:\", ridge.coef_, \"| intercept:\", ridge.intercept_)\n",
    "print(\"Stacked VALID RMSE:\", rmse_orig(y_valid, valid_blend))\n",
    "\n",
    "# TRAIN preds matrix for stacker\n",
    "P_train_list = []\n",
    "for idx in top_k:\n",
    "    params = trial_params[idx]\n",
    "    m = xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        eval_metric=\"rmse\",\n",
    "        random_state=SEED,\n",
    "        n_jobs=os.cpu_count(),\n",
    "        tree_method=\"hist\",\n",
    "        **params\n",
    "    )\n",
    "    # Fit on TRAIN split (same as gp trials) and predict TRAIN\n",
    "    m.fit(X_train, y_train_log, verbose=False)\n",
    "    p_tr = from_log(m.predict(X_train))\n",
    "    P_train_list.append(p_tr)\n",
    "\n",
    "P_train = np.column_stack(P_train_list).astype(np.float64)\n",
    "train_blend = ridge.predict(P_train)\n",
    "train_blend = np.maximum(train_blend, 0.0)\n",
    "\n",
    "# Build full training predictions (train + valid), with trained_on flag\n",
    "pred_train_df = pd.DataFrame({\n",
    "    \"idx\": train_df.index,\n",
    "    \"trained_on\": 1,\n",
    "    \"y_true\": y_train,\n",
    "    \"y_pred\": train_blend\n",
    "})\n",
    "pred_valid_df = pd.DataFrame({\n",
    "    \"idx\": valid_df.index,\n",
    "    \"trained_on\": 0,\n",
    "    \"y_true\": y_valid,\n",
    "    \"y_pred\": np.maximum(valid_blend, 0.0)\n",
    "})\n",
    "pred_full_df = pd.concat([pred_train_df, pred_valid_df], axis=0).sort_values(\"idx\").reset_index(drop=True)\n",
    "\n",
    "# Save full training predictions\n",
    "full_train_preds_path = RESULTS_DIR / f\"train_preds_{RUN_SUFFIX}.csv\"\n",
    "pred_full_df.to_csv(full_train_preds_path, index=False)\n",
    "print(\"Saved full training predictions:\", full_train_preds_path)\n",
    "\n",
    "# 7) Refit top models on FULL and predict TEST\n",
    "top_params_list = [trial_params[idx] for idx in top_k]\n",
    "P_test, model_bundle = refit_and_predict_full(df, dft, feat_cols, categorical_cols, top_params_list, SEED)\n",
    "\n",
    "# Blend TEST with ridge stacker\n",
    "test_blend = ridge.predict(P_test)\n",
    "test_blend = np.maximum(test_blend, 0.0)\n",
    "\n",
    "# 8) Save submission\n",
    "sub = pd.DataFrame({ID_COL: dft[ID_COL], TARGET_COL: test_blend.astype(float)})\n",
    "sub_path = SUBMISSIONS_DIR / f\"submission_{RUN_SUFFIX}.csv\"\n",
    "sub.to_csv(sub_path, index=False)\n",
    "print(\"Saved submission:\", sub_path)\n",
    "\n",
    "# 9) Save model bundle (preprocessor + top params + ridge)\n",
    "alphas_grid = getattr(ridge, \"alphas\", None)   # array or None\n",
    "alpha_best  = getattr(ridge, \"alpha_\", None)   # scalar or None\n",
    "model_artifact = {\n",
    "    \"bundle\": model_bundle,   # preprocessor_full, feat_cols, cat_feats, top_params, seed\n",
    "    \"stacker\": {\n",
    "        \"type\": \"RidgeCV\",\n",
    "        \"coef_\": ridge.coef_.tolist(),\n",
    "        \"intercept_\": float(ridge.intercept_),\n",
    "        \"alphas\": (alphas_grid.tolist() if alphas_grid is not None else None),\n",
    "        \"alpha_chosen\": (float(alpha_best) if alpha_best is not None else None),\n",
    "    },\n",
    "}\n",
    "pkl_path = DIR_MODEL / \"model.pkl\"\n",
    "with open(pkl_path, \"wb\") as f:\n",
    "    pickle.dump(model_artifact, f)\n",
    "print(\"Saved model:\", pkl_path)\n",
    "\n",
    "# 10) Save config/metadata JSON\n",
    "meta = {\n",
    "    \"seed\": SEED,\n",
    "    \"valid_size\": VALID_SIZE,\n",
    "    \"include_iicc\": INCLUDE_IICC,\n",
    "    \"include_text\": INCLUDE_TEXT,\n",
    "    \"n_calls\": N_CALLS,\n",
    "    \"n_initial_points\": N_INITIAL_POINTS,\n",
    "    \"n_best_models\": N_BEST_MODELS,\n",
    "    \"best_score\": float(res.fun),\n",
    "    \"best_params\": dict(zip(\n",
    "        [\"max_depth\",\"learning_rate\",\"n_estimators\",\"subsample\",\"colsample_bytree\",\n",
    "         \"min_child_weight\",\"reg_alpha\",\"reg_lambda\",\"gamma\"],\n",
    "        [int(y) if isinstance(y, (int, np.integer)) else float(y) for y in res.x]\n",
    "    )),\n",
    "    \"ridge_coef\": ridge.coef_.tolist(),\n",
    "    \"ridge_intercept\": float(ridge.intercept_),\n",
    "    \"features_used_count\": len(feat_cols),\n",
    "    \"run_suffix\": RUN_SUFFIX\n",
    "}\n",
    "json_path = DIR_MODEL / \"config.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(\"Saved config:\", json_path)\n",
    "\n",
    "# Final cleanup\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a8e8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
