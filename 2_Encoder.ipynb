{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46fc678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# SBERT Embedding Generation\n",
    "# Outputs (under ./embeddings)\n",
    "# ===============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "\n",
    "# --------------------\n",
    "# Config\n",
    "# --------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DATA_DIR = Path(\"./data\")\n",
    "EMB_DIR  = Path(\"./embeddings\"); EMB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_PATH = DATA_DIR / \"train_preprocessed.csv\"\n",
    "TEST_PATH  = DATA_DIR / \"test_preprocessed.csv\"\n",
    "\n",
    "SBERT_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "MODEL_TAG   = SBERT_MODEL.split(\"/\")[-1]                  # e.g. all-mpnet-base-v2\n",
    "\n",
    "EMB_MODEL_DIR = EMB_DIR / MODEL_TAG;    EMB_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RAW_DIR = EMB_MODEL_DIR / \"raw\";        RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PCA_DIR = EMB_MODEL_DIR / \"pca\";        PCA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SEV_DIR = EMB_MODEL_DIR / \"severity\";   SEV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EMB_NORMALIZE = True                 # normalize to unit length\n",
    "ADD_LENGTH_FEATURES = True           # append desc lengths in PCA parquet\n",
    "DO_PCA = True                        # toggle PCA parquet output\n",
    "N_COMPONENTS_PCA = 128               # num PCA components (fit on train, apply to test)\n",
    "\n",
    "ID_COL = \"ClaimNumber\"\n",
    "TEXT_COL = \"ClaimDescription\"\n",
    "\n",
    "# --------------------\n",
    "# Utilities\n",
    "# --------------------\n",
    "def normalize_text(s: pd.Series) -> pd.Series:\n",
    "    s = s.fillna(\"\").astype(str)\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    return s\n",
    "\n",
    "def to_numpy_backed(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # PCA cols -> float32\n",
    "    for c in [col for col in df.columns if col.startswith(\"text_pca_\")]:\n",
    "        df[c] = df[c].astype(np.float32)\n",
    "    # length features -> int32\n",
    "    for c in [\"desc_char_len\",\"desc_token_len\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(np.int32)\n",
    "    if ID_COL in df.columns:\n",
    "        df[ID_COL] = df[ID_COL].astype(str)\n",
    "    # ensure numpy-backed dtypes (avoid Arrow ext types)\n",
    "    try:\n",
    "        df = df.convert_dtypes(dtype_backend=\"numpy_nullable\")\n",
    "    except TypeError:\n",
    "        pass\n",
    "    return df\n",
    "\n",
    "def load_data(train_path: Path, test_path: Path) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    tr = pd.read_csv(train_path)\n",
    "    te = pd.read_csv(test_path)\n",
    "    for frame in (tr, te):\n",
    "        frame.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    assert ID_COL in tr.columns and ID_COL in te.columns, f\"Missing {ID_COL}\"\n",
    "    assert TEXT_COL in tr.columns and TEXT_COL in te.columns, f\"Missing {TEXT_COL}\"\n",
    "    return tr, te\n",
    "\n",
    "def build_lengths(df: pd.DataFrame, src_col: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    char_len = df[src_col].str.len().astype(np.int32).values\n",
    "    tok_len  = df[src_col].str.split().str.len().astype(np.int32).values\n",
    "    return char_len, tok_len\n",
    "\n",
    "def encode_texts(model: SentenceTransformer, texts: pd.Series,\n",
    "                 batch_size: int, normalize: bool) -> np.ndarray:\n",
    "    embs = model.encode(\n",
    "        texts.tolist(),\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=normalize,\n",
    "    )\n",
    "    return embs.astype(np.float32)\n",
    "\n",
    "def run_pca(train_emb: np.ndarray, test_emb: np.ndarray,\n",
    "            n_components: int, seed: int) -> tuple[np.ndarray, np.ndarray, PCA, float]:\n",
    "    pca = PCA(n_components=n_components, svd_solver=\"auto\", random_state=seed)\n",
    "    Xtr = pca.fit_transform(train_emb)\n",
    "    Xte = pca.transform(test_emb)\n",
    "    return Xtr.astype(np.float32), Xte.astype(np.float32), pca, float(pca.explained_variance_ratio_.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb5a3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SBERT: sentence-transformers/all-mpnet-base-v2 | Emb dim: 768 | Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 106/106 [00:39<00:00,  2.71it/s]\n",
      "Batches: 100%|██████████| 71/71 [00:26<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: train (54000, 768), test (36000, 768)\n",
      "Saved raw embeddings: embeddings\\all-mpnet-base-v2\\raw\\sbert_train.npy, embeddings\\all-mpnet-base-v2\\raw\\sbert_test.npy\n",
      "PCA: n_components=128 | explained_variance_sum=0.9144\n",
      "Saved PCA parquet: embeddings\\all-mpnet-base-v2\\pca\\text_pca_train.parquet, embeddings\\all-mpnet-base-v2\\pca\\text_pca_test.parquet\n",
      "[text_severity] OOF: RMSE(log)=1.1641 | R2=0.4178 | RMSE(orig)=33,381.98\n",
      "Saved severity parquet: embeddings\\all-mpnet-base-v2\\severity\\text_severity_oof.parquet, embeddings\\all-mpnet-base-v2\\severity\\text_severity_test.parquet\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# Orchestration\n",
    "# --------------------\n",
    "# 1) Load & normalize text\n",
    "train, test = load_data(TRAIN_PATH, TEST_PATH)\n",
    "train[f\"{TEXT_COL}_norm\"] = normalize_text(train[TEXT_COL])\n",
    "test[f\"{TEXT_COL}_norm\"]  = normalize_text(test[TEXT_COL])\n",
    "\n",
    "if ADD_LENGTH_FEATURES:\n",
    "    tr_char, tr_tok = build_lengths(train, f\"{TEXT_COL}_norm\")\n",
    "    te_char, te_tok = build_lengths(test, f\"{TEXT_COL}_norm\")\n",
    "\n",
    "# 2) Load encoder model\n",
    "model = SentenceTransformer(SBERT_MODEL, device=DEVICE)\n",
    "emb_dim = model.get_sentence_embedding_dimension()\n",
    "print(f\"Using SBERT: {SBERT_MODEL} | Emb dim: {emb_dim} | Device: {DEVICE}\")\n",
    "\n",
    "# 3) Encode\n",
    "train_emb = encode_texts(model, train[f\"{TEXT_COL}_norm\"], BATCH_SIZE, EMB_NORMALIZE)\n",
    "test_emb  = encode_texts(model, test[f\"{TEXT_COL}_norm\"],  BATCH_SIZE, EMB_NORMALIZE)\n",
    "print(f\"Embeddings: train {train_emb.shape}, test {test_emb.shape}\")\n",
    "\n",
    "# 4) Save raw embeddings (.npy)\n",
    "np.save(RAW_DIR / f\"sbert_train.npy\", train_emb)\n",
    "np.save(RAW_DIR / f\"sbert_test.npy\",  test_emb)\n",
    "print(f\"Saved raw embeddings: {RAW_DIR / f'sbert_train.npy'}, {RAW_DIR / f'sbert_test.npy'}\")\n",
    "\n",
    "# 5) Optional PCA → parquet with ClaimNumber (+ optional length features)\n",
    "if DO_PCA:\n",
    "    tr_pca, te_pca, pca, expl = run_pca(train_emb, test_emb, N_COMPONENTS_PCA, SEED)\n",
    "    print(f\"PCA: n_components={N_COMPONENTS_PCA} | explained_variance_sum={expl:.4f}\")\n",
    "\n",
    "    pca_cols = [f\"text_pca_{i}\" for i in range(N_COMPONENTS_PCA)]\n",
    "\n",
    "    pca_tr_df = pd.DataFrame(tr_pca, columns=pca_cols)\n",
    "    pca_tr_df.insert(0, ID_COL, train[ID_COL].astype(str).values)\n",
    "    pca_te_df = pd.DataFrame(te_pca, columns=pca_cols)\n",
    "    pca_te_df.insert(0, ID_COL, test[ID_COL].astype(str).values)\n",
    "\n",
    "    if ADD_LENGTH_FEATURES:\n",
    "        pca_tr_df[\"desc_char_len\"] = tr_char\n",
    "        pca_tr_df[\"desc_token_len\"] = tr_tok\n",
    "        pca_te_df[\"desc_char_len\"] = te_char\n",
    "        pca_te_df[\"desc_token_len\"] = te_tok\n",
    "\n",
    "    pca_tr_df = to_numpy_backed(pca_tr_df)\n",
    "    pca_te_df = to_numpy_backed(pca_te_df)\n",
    "\n",
    "    out_tr = PCA_DIR / f\"text_pca_train.parquet\"\n",
    "    out_te = PCA_DIR / f\"text_pca_test.parquet\"\n",
    "    pca_tr_df.to_parquet(out_tr, engine=\"pyarrow\", index=False)\n",
    "    pca_te_df.to_parquet(out_te, engine=\"pyarrow\", index=False)\n",
    "    print(f\"Saved PCA parquet: {out_tr}, {out_te}\")\n",
    "\n",
    "# 6) Text severity (Ridge OOF on logUICC)\n",
    "X = tr_pca\n",
    "Xtest = te_pca\n",
    "y = train[\"logUICC\"].values\n",
    "y_strat = train[\"accident_year\"].astype(int).values\n",
    "\n",
    "RIDGE_ALPHAS = np.logspace(-2, 2, 9)  # [0.01..100]\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof = np.zeros(len(train), dtype=np.float32)\n",
    "test_pred_folds = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y_strat), 1):\n",
    "    Xtr, ytr = X[tr_idx], y[tr_idx]\n",
    "    Xva, yva = X[va_idx], y[va_idx]\n",
    "\n",
    "    rc = RidgeCV(alphas=RIDGE_ALPHAS, cv=3, scoring=\"neg_mean_squared_error\")\n",
    "    rc.fit(Xtr, ytr)\n",
    "    alpha = float(rc.alpha_)\n",
    "\n",
    "    mdl = Ridge(alpha=alpha, random_state=SEED)\n",
    "    mdl.fit(Xtr, ytr)\n",
    "\n",
    "    oof[va_idx] = mdl.predict(Xva).astype(np.float32)\n",
    "    test_pred_folds.append(mdl.predict(Xtest).astype(np.float32))\n",
    "\n",
    "# OOF diagnostics\n",
    "oof_rmse_log = root_mean_squared_error(y, oof)\n",
    "oof_rmse_u   = root_mean_squared_error(np.expm1(y), np.expm1(oof))\n",
    "oof_r2       = r2_score(y, oof)\n",
    "print(f\"[text_severity] OOF: RMSE(log)={oof_rmse_log:.4f} | R2={oof_r2:.4f} | RMSE(orig)={oof_rmse_u:,.2f}\")\n",
    "\n",
    "# full-train model and test preds\n",
    "rc_full = RidgeCV(alphas=RIDGE_ALPHAS, cv=5, scoring=\"neg_mean_squared_error\").fit(X, y)\n",
    "ridge_full = Ridge(alpha=float(rc_full.alpha_), random_state=SEED).fit(X, y)\n",
    "test_pred_full = ridge_full.predict(Xtest).astype(np.float32)\n",
    "\n",
    "test_pred_stack = np.mean(np.stack(test_pred_folds, axis=0), axis=0).astype(np.float32)\n",
    "\n",
    "# Save severity parquet\n",
    "sev_oof_df = pd.DataFrame({ID_COL: train[ID_COL].astype(str).values, \"text_sev_oof\": oof})\n",
    "sev_te_df  = pd.DataFrame({ID_COL: test[ID_COL].astype(str).values,  \"text_sev_pred\": test_pred_stack})\n",
    "\n",
    "sev_oof_df.to_parquet(SEV_DIR / \"text_severity_oof.parquet\", index=False)\n",
    "sev_te_df.to_parquet(SEV_DIR / \"text_severity_test.parquet\", index=False)\n",
    "print(f\"Saved severity parquet: {SEV_DIR / 'text_severity_oof.parquet'}, {SEV_DIR / 'text_severity_test.parquet'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb2b193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
